{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cba6a086",
   "metadata": {},
   "source": [
    "# GraphRAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57a39d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import textwrap\n",
    "import tiktoken\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "# --- GraphRAG imports (API + utils) ---\n",
    "import graphrag.api as api\n",
    "from graphrag.config.load_config import load_config\n",
    "from graphrag.vector_stores.lancedb import LanceDBVectorStore\n",
    "from graphrag.vector_stores.base import VectorStoreDocument\n",
    "from graphrag.config.enums import ModelType\n",
    "from graphrag.config.models.language_model_config import LanguageModelConfig\n",
    "from graphrag.language_model.manager import ModelManager\n",
    "from graphrag.query.context_builder.entity_extraction import EntityVectorStoreKey\n",
    "\n",
    "# (Para local search con clases de bajo nivel)\n",
    "from graphrag.query.structured_search.local_search.search import LocalSearch\n",
    "from graphrag.query.structured_search.local_search.mixed_context import LocalSearchMixedContext\n",
    "from graphrag.query.indexer_adapters import (\n",
    "    read_indexer_entities,\n",
    "    read_indexer_relationships,\n",
    "    read_indexer_reports,\n",
    "    read_indexer_text_units,\n",
    "    read_indexer_covariates,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ce57e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 1) Preparar workspace de GraphRAG\n",
    "# ------------------------------------------------------------\n",
    "LEVEL = 2  # usar el mismo que en el global_search\n",
    "ROOT = Path(\"./graphrag_ws\").resolve()\n",
    "INPUT = ROOT / \"input\"\n",
    "OUTPUT = ROOT / \"output\"\n",
    "CACHE = ROOT / \"cache\"\n",
    "LANCEDB = ROOT / \"lancedb\"\n",
    "LANCEDB_URI = (ROOT / \"lancedb\").as_posix()\n",
    "ROOT.mkdir(parents=True, exist_ok=True)\n",
    "INPUT.mkdir(exist_ok=True); OUTPUT.mkdir(exist_ok=True)\n",
    "CACHE.mkdir(exist_ok=True); LANCEDB.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8fc0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muestra ejemplo de texto de entrada (puedes poner tus .txt en ./input)\n",
    "sample = textwrap.dedent(\"\"\"\n",
    "    Charles Dickens wrote A Christmas Carol. Scrooge is visited by three ghosts.\n",
    "    Bob Cratchit works for Scrooge. Tiny Tim is Bob Cratchit's son.\n",
    "\"\"\").strip()\n",
    "(INPUT / \"demo.txt\").write_text(sample, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a47e4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings.yaml mínimo (OpenAI). Para Azure OpenAI, sustituye el bloque models según docs.\n",
    "# Docs de configuración: modelos / input / output / vector_store (lancedb por defecto). \n",
    "# https://microsoft.github.io/graphrag/config/yaml/\n",
    "settings = f\"\"\"\n",
    "models:\n",
    "  default_chat_model:\n",
    "    api_key: ${{GRAPHRAG_API_KEY}}\n",
    "    type: openai_chat\n",
    "    model: gpt-4o\n",
    "    model_supports_json: true\n",
    "  default_embedding_model:\n",
    "    api_key: ${{GRAPHRAG_API_KEY}}\n",
    "    type: openai_embedding\n",
    "    model: text-embedding-3-large\n",
    "input:\n",
    "  type: file\n",
    "  base_dir: input\n",
    "  file_type: text\n",
    "chunks:\n",
    "  size: 1200\n",
    "  overlap: 150\n",
    "output:\n",
    "  type: file\n",
    "  base_dir: output\n",
    "cache:\n",
    "  type: file\n",
    "  base_dir: cache\n",
    "vector_store:\n",
    "  default_vector_store:\n",
    "    type: lancedb\n",
    "    db_uri: {LANCEDB.as_posix()}\n",
    "    container_name: default\n",
    "\"\"\"\n",
    "(ROOT / \"settings.yaml\").write_text(settings.strip(), encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf99c103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 2) Indexar con la API de GraphRAG (equivale a `graphrag index --root ...`)\n",
    "# ------------------------------------------------------------\n",
    "async def build_index():\n",
    "    cfg = load_config(ROOT)  # lee settings.yaml + .env\n",
    "    run_results = await api.build_index(config=cfg)  # lista de workflows ejecutados\n",
    "    for wf in run_results:\n",
    "        print(f\"[INDEX] {wf.workflow}: {'OK' if not wf.errors else 'ERROR'}\")\n",
    "        if wf.errors:\n",
    "            for e in wf.errors: print(\"   ->\", e)\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3de7c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ensure_entity_description_embeddings(entities_df, store, embedder):\n",
    "    \"\"\"\n",
    "    Crea (si no existe) y rellena la colección de LanceDB para descripciones de entidades.\n",
    "    - Usa la columna 'description_embedding' si existe; si no, calcula embeddings con embedder.\n",
    "    - 'store' es tu LanceDBVectorStore ya conectado.\n",
    "    \"\"\"\n",
    "    # Si la colección no está creada, load_documents la creará.\n",
    "    docs = []\n",
    "    # Normaliza nombres de columnas por si cambian mayúsculas/minúsculas\n",
    "    cols = {c.lower(): c for c in entities_df.columns}\n",
    "    col_id = cols.get(\"id\", \"id\")\n",
    "    col_title = cols.get(\"title\", \"title\")\n",
    "    col_desc = cols.get(\"description\", \"description\")\n",
    "    col_desc_emb = cols.get(\"description_embedding\")  # puede no existir\n",
    "\n",
    "    for _, row in entities_df.iterrows():\n",
    "        ent_id = str(row[col_id])\n",
    "        title = str(row.get(col_title, \"\") or \"\")\n",
    "        desc = str(row.get(col_desc, \"\") or \"\")\n",
    "        text = (desc or title).strip()\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        vec = None\n",
    "        if col_desc_emb and row.get(col_desc_emb) is not None:\n",
    "            vec = row[col_desc_emb]\n",
    "        if vec is None:\n",
    "            # calcula embedding sincrónicamente (tu LocalSearch usa API sync igualmente)\n",
    "            vec = embedder.embed(text)\n",
    "\n",
    "        docs.append(\n",
    "            VectorStoreDocument(\n",
    "                id=ent_id,\n",
    "                text=text,\n",
    "                vector=vec,\n",
    "                attributes={\"title\": title},\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if docs:\n",
    "        # Crea o sobreescribe la colección con estos documentos\n",
    "        store.load_documents(documents=docs, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fa3d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 3) Consultas\n",
    "#   a) Global Search (dataset‑wide, sobre community reports)\n",
    "#   b) Local Search (entity‑centric, mezcla KG + pasajes)\n",
    "#   Referencias API/notebooks: \n",
    "#     - Global: api.global_search + docs Query/Global Search\n",
    "#     - Local: clases LocalSearch + LocalSearchMixedContext + notebooks\n",
    "# ------------------------------------------------------------\n",
    "async def run_queries(cfg):\n",
    "    # --- GLOBAL SEARCH (si ya lo tienes OK, déjalo igual) ---\n",
    "    entities_df = pd.read_parquet(OUTPUT / \"entities.parquet\")\n",
    "    communities_df = pd.read_parquet(OUTPUT / \"communities.parquet\")\n",
    "    community_reports_df = pd.read_parquet(OUTPUT / \"community_reports.parquet\")\n",
    "\n",
    "    response_glob, context_glob = await api.global_search(\n",
    "        config=cfg,\n",
    "        entities=entities_df,\n",
    "        communities=communities_df,\n",
    "        community_reports=community_reports_df,\n",
    "        community_level=LEVEL,\n",
    "        dynamic_community_selection=False,\n",
    "        response_type=\"Multiple Paragraphs\",\n",
    "        query=\"¿Cuáles son los temas principales del texto?\",\n",
    "    )\n",
    "    print(\"\\n=== GLOBAL SEARCH ===\")\n",
    "    print(response_glob)\n",
    "\n",
    "    # --- LOCAL SEARCH ---\n",
    "    # 1) DataFrames de todos los artefactos\n",
    "    entity_df = entities_df\n",
    "    community_df = communities_df\n",
    "    relationship_df = pd.read_parquet(OUTPUT / \"relationships.parquet\")\n",
    "    report_df = community_reports_df\n",
    "    text_unit_df = pd.read_parquet(OUTPUT / \"text_units.parquet\")\n",
    "\n",
    "    # (opcional)\n",
    "    try:\n",
    "        covariate_df = pd.read_parquet(OUTPUT / \"covariates.parquet\")\n",
    "    except FileNotFoundError:\n",
    "        covariate_df = None\n",
    "\n",
    "    # 2) Adaptadores TIPADOS (fíjate en las firmas)\n",
    "    entities = read_indexer_entities(entity_df, community_df, LEVEL)\n",
    "    relationships = read_indexer_relationships(relationship_df)\n",
    "    reports = read_indexer_reports(report_df, community_df, LEVEL)\n",
    "    text_units = read_indexer_text_units(text_unit_df)\n",
    "    covariates = {\"claims\": read_indexer_covariates(covariate_df)} if covariate_df is not None else None\n",
    "\n",
    "    api_key = os.environ[\"GRAPHRAG_API_KEY\"]\n",
    "    llm_model = \"gpt-4o\"               # opcional, solo para token_encoder; puedes leerlo del cfg\n",
    "    embedding_model = \"text-embedding-3-large\"  # debe coincidir con tu settings.yaml\n",
    "\n",
    "    LANCEDB_URI = LANCEDB.as_posix()\n",
    "\n",
    "    description_embedding_store = LanceDBVectorStore(\n",
    "        collection_name=\"entity_description_embeddings\"  # nombre estándar que usa el indexer\n",
    "    )\n",
    "    description_embedding_store.connect(db_uri=LANCEDB_URI)\n",
    "\n",
    "    embed_config = LanguageModelConfig(\n",
    "        api_key=api_key,\n",
    "        type=ModelType.OpenAIEmbedding,\n",
    "        model=embedding_model,\n",
    "        max_retries=20,\n",
    "    )\n",
    "\n",
    "    mm = ModelManager()\n",
    "    text_embedder = mm.get_or_create_embedding_model(\n",
    "        name=\"local_search_embedding\",\n",
    "        model_type=ModelType.OpenAIEmbedding,\n",
    "        config=embed_config,\n",
    "    )\n",
    "    token_encoder = tiktoken.encoding_for_model(llm_model)  # o tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    chat_type = (cfg.models[\"default_chat_model\"].type).lower()\n",
    "    chat_model_name = cfg.models[\"default_chat_model\"].model\n",
    "\n",
    "    if chat_type == \"openai_chat\":\n",
    "        mt = ModelType.OpenAIChat\n",
    "        chat_config = LanguageModelConfig(\n",
    "            api_key=api_key,\n",
    "            type=mt,\n",
    "            model=chat_model_name,\n",
    "            max_retries=20,\n",
    "        )\n",
    "    elif chat_type == \"azure_openai_chat\":\n",
    "        mt = ModelType.AzureOpenAIChat\n",
    "        chat_config = LanguageModelConfig(\n",
    "            api_key=api_key,\n",
    "            type=mt,\n",
    "            model=chat_model_name,                               # opcional según tu setup\n",
    "            api_base=cfg[\"models\"][\"default_chat_model\"][\"api_base\"],\n",
    "            api_version=cfg[\"models\"][\"default_chat_model\"][\"api_version\"],\n",
    "            deployment_name=cfg[\"models\"][\"default_chat_model\"][\"deployment_name\"],\n",
    "            max_retries=20,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Tipo de chat no soportado: {chat_type}\")\n",
    "\n",
    "    chat_model = mm.get_or_create_chat_model(\n",
    "        name=\"local_search_chat\",\n",
    "        model_type=mt,\n",
    "        config=chat_config,\n",
    "    )\n",
    "\n",
    "    # --- LLAMADA: crea la colección si falta ---\n",
    "    if getattr(description_embedding_store, \"document_collection\", None) is None:\n",
    "        print(\"[INFO] Creando colección de embeddings de entidades en LanceDB…\")\n",
    "        _ensure_entity_description_embeddings(entities_df, description_embedding_store, text_embedder)\n",
    "\n",
    "    # Sonda rápida: ahora debe existir y responder\n",
    "    probe_vec = text_embedder.embed(\"Scrooge\")\n",
    "    _ = description_embedding_store.similarity_search_by_vector(probe_vec, k=1)\n",
    "    print(\"[OK] La colección de embeddings de entidades responde a búsquedas.\")\n",
    "\n",
    "    # 3) Contexto y búsqueda local\n",
    "    ctx_builder = LocalSearchMixedContext(\n",
    "        community_reports=reports,\n",
    "        text_units=text_units,\n",
    "        entities=entities,\n",
    "        relationships=relationships,\n",
    "        covariates=covariates,  # o None si no tienes\n",
    "        entity_text_embeddings=description_embedding_store,\n",
    "        # La mayoría de builds usan el ID interno de la entidad para indexar las embeddings:\n",
    "        embedding_vectorstore_key=EntityVectorStoreKey.ID,\n",
    "        # Si las indexaste por título de entidad, usa: EntityVectorStoreKey.TITLE\n",
    "        text_embedder=text_embedder,\n",
    "        token_encoder=token_encoder,\n",
    "    )\n",
    "    local = LocalSearch(model=chat_model, context_builder=ctx_builder)\n",
    "\n",
    "    print(f\"Local search object: {local}\")\n",
    "\n",
    "    result_local = await local.search(\n",
    "        query=\"¿Quién es Scrooge y qué relaciones clave tiene?\"\n",
    "    )\n",
    "    print(\"\\n=== LOCAL SEARCH ===\")\n",
    "    print(result_local.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac06551",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    cfg = await build_index()\n",
    "    await run_queries(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ae6e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70b02d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
