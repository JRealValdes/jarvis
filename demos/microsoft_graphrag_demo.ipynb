{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cba6a086",
   "metadata": {},
   "source": [
    "# Microsoft's GraphRAG demo\n",
    "GraphRAG minimal end-to-end example:\n",
    "1) Prepare a workspace and minimal settings.yaml\n",
    "2) Build the index (same as `graphrag index --root ...`)\n",
    "3) Run queries:\n",
    "   - Global Search (dataset-wide, uses community reports)\n",
    "   - Local Search (entity-centric, mixes KG + passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57a39d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import textwrap\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "\n",
    "# --- GraphRAG imports (API + utils) ---\n",
    "import graphrag.api as api\n",
    "from graphrag.config.load_config import load_config\n",
    "from graphrag.vector_stores.lancedb import LanceDBVectorStore\n",
    "from graphrag.vector_stores.base import VectorStoreDocument\n",
    "from graphrag.config.enums import ModelType\n",
    "from graphrag.config.models.language_model_config import LanguageModelConfig\n",
    "from graphrag.language_model.manager import ModelManager\n",
    "from graphrag.query.context_builder.entity_extraction import EntityVectorStoreKey\n",
    "\n",
    "# Local search (structured search) components\n",
    "from graphrag.query.structured_search.local_search.search import LocalSearch\n",
    "from graphrag.query.structured_search.local_search.mixed_context import (\n",
    "    LocalSearchMixedContext,\n",
    ")\n",
    "from graphrag.query.indexer_adapters import (\n",
    "    read_indexer_entities,\n",
    "    read_indexer_relationships,\n",
    "    read_indexer_reports,\n",
    "    read_indexer_text_units,\n",
    "    read_indexer_covariates,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ce57e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1) WORKSPACE PREPARATION\n",
    "# =============================================================================\n",
    "\n",
    "# Community level to use for global/local search.\n",
    "# It determines the level of detail while forming communities in the KG index.\n",
    "# Must match your indexed outputs.\n",
    "LEVEL: int = 2\n",
    "\n",
    "# Project layout (root folders for GraphRAG)\n",
    "ROOT = Path(\"./graphrag_ws\").resolve()\n",
    "INPUT = ROOT / \"input\"\n",
    "OUTPUT = ROOT / \"output\"\n",
    "CACHE = ROOT / \"cache\"\n",
    "LANCEDB = ROOT / \"lancedb\"\n",
    "LANCEDB_URI = LANCEDB.as_posix()\n",
    "\n",
    "# Ensure folders exist\n",
    "ROOT.mkdir(parents=True, exist_ok=True)\n",
    "INPUT.mkdir(exist_ok=True)\n",
    "OUTPUT.mkdir(exist_ok=True)\n",
    "CACHE.mkdir(exist_ok=True)\n",
    "LANCEDB.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8fc0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide a tiny input sample (you can drop .txt files in ./input instead)\n",
    "sample = textwrap.dedent(\n",
    "    \"\"\"\n",
    "    Charles Dickens wrote \"A Christmas Carol\". Scrooge is visited by three ghosts.\n",
    "    Bob Cratchit works for Scrooge. Tiny Tim is Bob Cratchit's son.\n",
    "    \"\"\"\n",
    ").strip()\n",
    "(INPUT / \"demo.txt\").write_text(sample, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a47e4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal settings.yaml (OpenAI). For Azure OpenAI, switch model types & fields accordingly.\n",
    "# Config docs: https://microsoft.github.io/graphrag/config/yaml/\n",
    "settings = f\"\"\"\n",
    "models:\n",
    "  default_chat_model:\n",
    "    api_key: ${{GRAPHRAG_API_KEY}}\n",
    "    type: openai_chat\n",
    "    model: gpt-4o\n",
    "    model_supports_json: true\n",
    "  default_embedding_model:\n",
    "    api_key: ${{GRAPHRAG_API_KEY}}\n",
    "    type: openai_embedding\n",
    "    model: text-embedding-3-large\n",
    "\n",
    "input:\n",
    "  type: file\n",
    "  base_dir: input\n",
    "  file_type: text\n",
    "\n",
    "chunks:\n",
    "  size: 1200\n",
    "  overlap: 150\n",
    "\n",
    "output:\n",
    "  type: file\n",
    "  base_dir: output\n",
    "\n",
    "cache:\n",
    "  type: file\n",
    "  base_dir: cache\n",
    "\n",
    "vector_store:\n",
    "  default_vector_store:\n",
    "    type: lancedb\n",
    "    db_uri: {LANCEDB_URI}\n",
    "    container_name: default\n",
    "\"\"\"\n",
    "(ROOT / \"settings.yaml\").write_text(settings.strip(), encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf99c103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2) INDEX BUILD (equivalent to `graphrag index --root ...`)\n",
    "# =============================================================================\n",
    "async def build_index():\n",
    "    \"\"\"\n",
    "    Load settings + environment, then run the GraphRAG index workflows.\n",
    "    Prints a per-workflow status line and returns the loaded config object.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cfg : GraphRAG config object\n",
    "    \"\"\"\n",
    "    cfg = load_config(ROOT)\n",
    "    run_results = await api.build_index(config=cfg)\n",
    "    for wf in run_results:\n",
    "        print(f\"[INDEX] {wf.workflow}: {'OK' if not wf.errors else 'ERROR'}\")\n",
    "        if wf.errors:\n",
    "            for e in wf.errors:\n",
    "                print(\"   ->\", e)\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3de7c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Helper: ensure entity description embeddings exist in LanceDB\n",
    "# =============================================================================\n",
    "def _ensure_entity_description_embeddings(\n",
    "    entities_df: pd.DataFrame,\n",
    "    store: LanceDBVectorStore,\n",
    "    embedder,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Create (if missing) and populate a LanceDB collection for entity description embeddings.\n",
    "\n",
    "    Why this is needed:\n",
    "    - LocalSearch can use a vector store of *entity descriptions* to retrieve entities by\n",
    "      semantic similarity (e.g., match a query to \"Scrooge\" description).\n",
    "    - Index builds sometimes already include `description_embedding`. If not present,\n",
    "      we compute embeddings on the fly.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    entities_df : pd.DataFrame\n",
    "        DataFrame produced by the indexer (entities.parquet).\n",
    "    store : LanceDBVectorStore\n",
    "        A connected LanceDB vector store pointing to the collection we want to (over)write.\n",
    "    embedder : Embedding model (from ModelManager)\n",
    "        Must have an `embed(text: str) -> List[float]` method.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - This function overwrites the collection contents for a clean demo.\n",
    "      In production, prefer upserts/merges and versioning.\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "\n",
    "    # Normalize column names to be resilient to case changes\n",
    "    cols = {c.lower(): c for c in entities_df.columns}\n",
    "    col_id = cols.get(\"id\", \"id\")\n",
    "    col_title = cols.get(\"title\", \"title\")\n",
    "    col_desc = cols.get(\"description\", \"description\")\n",
    "    col_desc_emb = cols.get(\"description_embedding\")  # may not exist\n",
    "\n",
    "    for _, row in entities_df.iterrows():\n",
    "        ent_id = str(row[col_id])\n",
    "        title = str(row.get(col_title, \"\") or \"\")\n",
    "        desc = str(row.get(col_desc, \"\") or \"\")\n",
    "        text = (desc or title).strip()\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        vec = None\n",
    "        if col_desc_emb and row.get(col_desc_emb) is not None:\n",
    "            vec = row[col_desc_emb]\n",
    "        if vec is None:\n",
    "            # Compute embedding (sync). LocalSearch also uses sync LLM/embedding calls.\n",
    "            vec = embedder.embed(text)\n",
    "\n",
    "        docs.append(\n",
    "            VectorStoreDocument(\n",
    "                id=ent_id,\n",
    "                text=text,\n",
    "                vector=vec,\n",
    "                attributes={\"title\": title},\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if docs:\n",
    "        # Create or overwrite the collection with these documents\n",
    "        store.load_documents(documents=docs, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fa3d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3) QUERIES: Global + Local\n",
    "# =============================================================================\n",
    "async def run_queries(cfg) -> None:\n",
    "    \"\"\"\n",
    "    Demonstrates:\n",
    "    - Global Search over community reports\n",
    "    - Local Search using a mixed context (KG + text) and entity description embeddings\n",
    "    \"\"\"\n",
    "    # --- Load index artifacts (DataFrames) produced by the index build ---\n",
    "    entities_df = pd.read_parquet(OUTPUT / \"entities.parquet\")\n",
    "    communities_df = pd.read_parquet(OUTPUT / \"communities.parquet\")\n",
    "    community_reports_df = pd.read_parquet(OUTPUT / \"community_reports.parquet\")\n",
    "    relationship_df = pd.read_parquet(OUTPUT / \"relationships.parquet\")\n",
    "    text_unit_df = pd.read_parquet(OUTPUT / \"text_units.parquet\")\n",
    "\n",
    "    # Optional covariates\n",
    "    try:\n",
    "        covariate_df = pd.read_parquet(OUTPUT / \"covariates.parquet\")\n",
    "    except FileNotFoundError:\n",
    "        covariate_df = None\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 3a) GLOBAL SEARCH (dataset-wide, uses community reports)\n",
    "    # ---------------------------------------------------------------------\n",
    "    response_glob, context_glob = await api.global_search(\n",
    "        config=cfg,\n",
    "        entities=entities_df,\n",
    "        communities=communities_df,\n",
    "        community_reports=community_reports_df,\n",
    "        community_level=LEVEL,\n",
    "        dynamic_community_selection=False,\n",
    "        response_type=\"Multiple Paragraphs\",\n",
    "        query=\"What are the main topics covered in the text?\",\n",
    "    )\n",
    "    print(\"\\n=== GLOBAL SEARCH ===\")\n",
    "    print(response_glob)\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 3b) LOCAL SEARCH (entity-centric; mixes KG + passages)\n",
    "    #\n",
    "    # Pipeline overview:\n",
    "    # 1) Convert indexer DataFrames into typed adapters (expected by LocalSearch)\n",
    "    # 2) Prepare an entity description embedding store (LanceDB)\n",
    "    # 3) Create a MixedContext builder that combines:\n",
    "    #       - community reports (summaries)\n",
    "    #       - text units (passages)\n",
    "    #       - entities and relationships (KG)\n",
    "    #       - optional covariates (claims, etc.)\n",
    "    #       - entity description embeddings (for entity recall)\n",
    "    # 4) Run a question against LocalSearch and print the response\n",
    "    # ---------------------------------------------------------------------\n",
    "    entities = read_indexer_entities(entities_df, communities_df, LEVEL)\n",
    "    relationships = read_indexer_relationships(relationship_df)\n",
    "    reports = read_indexer_reports(community_reports_df, communities_df, LEVEL)\n",
    "    text_units = read_indexer_text_units(text_unit_df)\n",
    "    covariates = (\n",
    "        {\"claims\": read_indexer_covariates(covariate_df)} if covariate_df is not None else None\n",
    "    )\n",
    "\n",
    "    # --- Models and token encoder ---\n",
    "    api_key = os.environ[\"GRAPHRAG_API_KEY\"]\n",
    "\n",
    "    # Use the same model names as in settings.yaml\n",
    "    llm_model = \"gpt-4o\"\n",
    "    embedding_model = \"text-embedding-3-large\"\n",
    "\n",
    "    # Prepare LanceDB store for entity description embeddings\n",
    "    description_embedding_store = LanceDBVectorStore(\n",
    "        collection_name=\"entity_description_embeddings\"  # same default used by indexers\n",
    "    )\n",
    "    description_embedding_store.connect(db_uri=LANCEDB_URI)\n",
    "\n",
    "    # Embedding model (via ModelManager)\n",
    "    embed_config = LanguageModelConfig(\n",
    "        api_key=api_key,\n",
    "        type=ModelType.OpenAIEmbedding,\n",
    "        model=embedding_model,\n",
    "        max_retries=20,\n",
    "    )\n",
    "    mm = ModelManager()\n",
    "    text_embedder = mm.get_or_create_embedding_model(\n",
    "        name=\"local_search_embedding\",\n",
    "        model_type=ModelType.OpenAIEmbedding,\n",
    "        config=embed_config,\n",
    "    )\n",
    "\n",
    "    # Token encoder for prompt budgeting / truncation\n",
    "    try:\n",
    "        token_encoder = tiktoken.encoding_for_model(llm_model)\n",
    "    except Exception:\n",
    "        # Fallback if the model name is unknown to tiktoken\n",
    "        token_encoder = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    # Select chat model type from loaded config\n",
    "    model_cfg = cfg.models[\"default_chat_model\"]\n",
    "    chat_type = str(model_cfg.type).lower()\n",
    "    chat_model_name = model_cfg.model\n",
    "\n",
    "    if chat_type == \"openai_chat\":\n",
    "        mt = ModelType.OpenAIChat\n",
    "        chat_config = LanguageModelConfig(\n",
    "            api_key=api_key,\n",
    "            type=mt,\n",
    "            model=chat_model_name,\n",
    "            max_retries=20,\n",
    "        )\n",
    "    elif chat_type == \"azure_openai_chat\":\n",
    "        mt = ModelType.AzureOpenAIChat\n",
    "        # For Azure, you must have these fields in settings.yaml's default_chat_model\n",
    "        chat_config = LanguageModelConfig(\n",
    "            api_key=api_key,\n",
    "            type=mt,\n",
    "            model=chat_model_name,  # optional depending on your setup\n",
    "            api_base=model_cfg.api_base,\n",
    "            api_version=model_cfg.api_version,\n",
    "            deployment_name=model_cfg.deployment_name,\n",
    "            max_retries=20,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported chat model type: {chat_type}\")\n",
    "\n",
    "    chat_model = mm.get_or_create_chat_model(\n",
    "        name=\"local_search_chat\",\n",
    "        model_type=mt,\n",
    "        config=chat_config,\n",
    "    )\n",
    "\n",
    "    # --- Ensure the LanceDB collection exists and is queryable ---\n",
    "    if getattr(description_embedding_store, \"document_collection\", None) is None:\n",
    "        print(\"[INFO] Creating entity description embedding collection in LanceDB…\")\n",
    "        _ensure_entity_description_embeddings(\n",
    "            entities_df=entities_df,\n",
    "            store=description_embedding_store,\n",
    "            embedder=text_embedder,\n",
    "        )\n",
    "\n",
    "    # Quick probe: a similarity search should succeed now\n",
    "    probe_vec = text_embedder.embed(\"Scrooge\")\n",
    "    _ = description_embedding_store.similarity_search_by_vector(probe_vec, k=1)\n",
    "    print(\"[OK] Entity description embedding collection is queryable.\")\n",
    "\n",
    "    # --- Build mixed context and run the local search ---\n",
    "    ctx_builder = LocalSearchMixedContext(\n",
    "        community_reports=reports,\n",
    "        text_units=text_units,\n",
    "        entities=entities,\n",
    "        relationships=relationships,\n",
    "        covariates=covariates,  # pass None if not available\n",
    "        entity_text_embeddings=description_embedding_store,\n",
    "        # Most builds use the internal entity ID as the vector key:\n",
    "        embedding_vectorstore_key=EntityVectorStoreKey.ID,\n",
    "        # If you indexed by entity title instead, use: EntityVectorStoreKey.TITLE\n",
    "        text_embedder=text_embedder,\n",
    "        token_encoder=token_encoder,\n",
    "    )\n",
    "\n",
    "    local = LocalSearch(model=chat_model, context_builder=ctx_builder)\n",
    "    print(f\"LocalSearch object ready: {local}\")\n",
    "\n",
    "    result_local = await local.search(\n",
    "        query=\"Who is Scrooge and what are his key relationships?\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n=== LOCAL SEARCH ===\")\n",
    "    print(result_local.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac06551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Entry point\n",
    "# =============================================================================\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    Full pipeline runner:\n",
    "      1) Build index from ./input into ./output\n",
    "      2) Run both Global and Local searches\n",
    "    \"\"\"\n",
    "    cfg = await build_index()\n",
    "    await run_queries(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ae6e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd8f9e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
