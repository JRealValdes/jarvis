{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cba6a086",
   "metadata": {},
   "source": [
    "# GraphRAG and LangGraph - Integration demo\n",
    "GraphRAG implemented using LlamaIndex and Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22656df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ---------- LlamaIndex (índices) ----------\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    Settings,\n",
    ")\n",
    "from llama_index.core.indices.knowledge_graph import KnowledgeGraphIndex\n",
    "from llama_index.core.indices.list.base import SummaryIndex\n",
    "from llama_index.core.composability import ComposableGraph\n",
    "from llama_index.graph_stores.neo4j import Neo4jGraphStore\n",
    "from llama_index.llms.openai import OpenAI as LI_OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "# ---------- LangGraph / LangChain (agente) ----------\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54286803",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "NEO4J_URL = os.getenv(\"NEO4J_URI\")\n",
    "NEO4J_USER = os.getenv(\"NEO4J_USER\", \"neo4j\")\n",
    "NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\")\n",
    "PDF_DIR = \"../data/docs\"\n",
    "\n",
    "# OpenAI models (adjustable)\n",
    "LI_LLM_MODEL = \"gpt-4o-mini\"    # for internal use of LlamaIndex\n",
    "EMBED_MODEL = \"text-embedding-3-small\"   # for embeddings\n",
    "AGENT_LLM_MODEL = \"gpt-4o-mini\"    # The LangGraph agent's LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e893483b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_indices():\n",
    "    # ---------- LlamaIndex Global Config ----------\n",
    "    Settings.llm = LI_OpenAI(model=LI_LLM_MODEL, temperature=0)\n",
    "    Settings.embed_model = OpenAIEmbedding(model=EMBED_MODEL)\n",
    "\n",
    "    # ---------- 1) PDFs → documents ----------\n",
    "    docs = SimpleDirectoryReader(input_dir=PDF_DIR).load_data()\n",
    "    if not docs:\n",
    "        raise RuntimeError(f\"PDFs not found in folder: {PDF_DIR}\")\n",
    "\n",
    "    # ---------- 2) Neo4j Graph ----------\n",
    "    graph_store = Neo4jGraphStore(\n",
    "        url=NEO4J_URL, username=NEO4J_USER, password=NEO4J_PASSWORD\n",
    "    )\n",
    "\n",
    "    # ---------- 3) Build KG (triplets → Neo4j) ----------\n",
    "    kg_index = KnowledgeGraphIndex.from_documents(\n",
    "        docs,\n",
    "        graph_store=graph_store,\n",
    "        max_triplets_per_chunk=3,\n",
    "    )\n",
    "\n",
    "    # ---------- 4) Vector index ----------\n",
    "    vec_index = VectorStoreIndex.from_documents(docs)\n",
    "\n",
    "    # ---------- 5) Hybrid compose - Composable Graph ----------\n",
    "    # NOTE: Do not mistake this for a \"GraphRAG\" agent; this is a composable index in the form of a graph.\n",
    "    # It allows querying both the KG and the vector index.\n",
    "    composed = ComposableGraph.from_indices(\n",
    "        root_index_cls=SummaryIndex,\n",
    "        children_indices=[kg_index, vec_index],\n",
    "        index_summaries=[\n",
    "            \"Knowledge Graph: entities and relations from the corpus; useful for relational queries or search for connections.\",\n",
    "            \"Vector Index: textual sections extracted from documents; useful for definitions, formulas and literal quotes.\"\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return kg_index, vec_index, composed\n",
    "\n",
    "\n",
    "def build_tools(kg_index, vec_index, composed):\n",
    "    # Independent Query engines\n",
    "    kg_engine = kg_index.as_query_engine()                 # focused on graph (entities/relations)\n",
    "    vec_engine = vec_index.as_query_engine(similarity_top_k=4)  # focused on textual citations\n",
    "    cmp_engine = composed.as_query_engine()                # hybryd (uses root-children hierarchy)\n",
    "\n",
    "    @tool\n",
    "    def graph_search(query: str) -> str:\n",
    "        \"\"\"Returns entities/relations extracted from the graph (Neo4j). It also includes textual citations.\"\"\"\n",
    "        print(\"Graph search query:\", query)\n",
    "        result = str(kg_engine.query(query))\n",
    "        print(\"Graph search result:\", result)\n",
    "        return result\n",
    "\n",
    "    @tool\n",
    "    def vector_search(query: str) -> str:\n",
    "        \"\"\"Returns relevant textual passages from the vector index.\"\"\"\n",
    "        print(\"Vector search query:\", query)\n",
    "        result = str(vec_engine.query(query))\n",
    "        print(\"Vector search result:\", result)\n",
    "        return result\n",
    "\n",
    "    @tool\n",
    "    def hybrid_search(query: str) -> str:\n",
    "        \"\"\"It combines graph and vector search to provide a more complete context.\"\"\"\n",
    "        print(\"Hybrid search query:\", query)\n",
    "        result = str(cmp_engine.query(query))\n",
    "        print(\"Hybrid search result:\", result)\n",
    "        return result\n",
    "\n",
    "    return [graph_search, vector_search, hybrid_search]\n",
    "\n",
    "\n",
    "def build_agent(tools):\n",
    "    # LLM for the agent (LangGraph / LangChain)\n",
    "    llm = ChatOpenAI(model=AGENT_LLM_MODEL, temperature=0)\n",
    "\n",
    "    system_prompt = (\n",
    "        \"Eres un agente GraphRAG. Decide entre graph_search (para entidades/relaciones), \"\n",
    "        \"vector_search (para pasajes textuales) o hybrid_search (para combinar). \"\n",
    "        \"Responde citando hechos claros y di si falta contexto.\"\n",
    "    )\n",
    "\n",
    "    agent = create_react_agent(\n",
    "        model=llm,\n",
    "        tools=tools,\n",
    "        prompt=system_prompt,\n",
    "    )\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562727ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_index, vec_index, composed = build_indices()\n",
    "tools = build_tools(kg_index, vec_index, composed)\n",
    "agent = build_agent(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae94c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual representation of the agent\n",
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54538d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use example questions to test the agent:\n",
    "questions = [\n",
    "    \"¿Qué problema resuelve el mecanismo de Multi-Head Attention y por qué es útil?\",\n",
    "    \"¿Cómo se calcula el scaled dot-product attention?\",\n",
    "    \"¿Qué ventajas ofrece el Transformer frente a modelos recurrentes en el paper? Usa un vector search para informarte.\",\n",
    "    \"¿Qué ventajas ofrece el Transformer frente a modelos recurrentes en el paper? Usa un graph search para informarte.\",\n",
    "    \"¿Qué ventajas ofrece el Transformer frente a modelos recurrentes en el paper? Usa un hybrid search para informarte.\",\n",
    "]\n",
    "for question in questions:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Q:\", question)\n",
    "    result = agent.invoke({\"messages\": [(\"user\", question)]})\n",
    "    print(\"A:\", [result[\"messages\"][-1].content] if result[\"messages\"] else \"<sin respuesta>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9931b314",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
